{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Investigation\n",
    "Experimental Data for Question Classification\n",
    "\n",
    "obtained from https://cogcomp.seas.upenn.edu/Data/QA/QC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and pre-processing\n",
    "\n",
    "Run this cell to download the dataset:\n",
    "\n",
    "Possibilies: `train_5500.label`, `train_4000.label`,`train_3000.label`,`train_2000.label`, `train_1000.label`,`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'datasets/train_5500.label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bdcabd1dba1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://cogcomp.seas.upenn.edu/Data/QA/QC/{filename}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'datasets/{filename}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'datasets/train_5500.label'"
     ]
    }
   ],
   "source": [
    "filename = 'train_5500.label'\n",
    "\n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "url = f'https://cogcomp.seas.upenn.edu/Data/QA/QC/{filename}'\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "with open(f'datasets/{filename}', 'xb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save queries and class labels to Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?']\n",
      "DESC:manner\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "label = []\n",
    "\n",
    "with open(f'datasets/{filename}', 'r', errors='replace') as data:\n",
    "    for line in data.readlines():\n",
    "        tokens = line.split()\n",
    "        label.append(tokens[0])\n",
    "        query.append(tokens[1:])\n",
    "        \n",
    "print(query[0])\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract classes and subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main class count: 6\n",
      "subclass count: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'DESC': ['manner', 'def', 'reason', 'desc'],\n",
       "             'ENTY': ['cremat',\n",
       "              'animal',\n",
       "              'event',\n",
       "              'other',\n",
       "              'letter',\n",
       "              'religion',\n",
       "              'food',\n",
       "              'color',\n",
       "              'termeq',\n",
       "              'body',\n",
       "              'dismed',\n",
       "              'product',\n",
       "              'substance',\n",
       "              'sport',\n",
       "              'plant',\n",
       "              'techmeth',\n",
       "              'instru',\n",
       "              'word',\n",
       "              'lang',\n",
       "              'symbol',\n",
       "              'veh',\n",
       "              'currency'],\n",
       "             'ABBR': ['exp', 'abb'],\n",
       "             'HUM': ['ind', 'gr', 'title', 'desc'],\n",
       "             'NUM': ['date',\n",
       "              'count',\n",
       "              'money',\n",
       "              'period',\n",
       "              'volsize',\n",
       "              'other',\n",
       "              'speed',\n",
       "              'perc',\n",
       "              'code',\n",
       "              'dist',\n",
       "              'temp',\n",
       "              'ord',\n",
       "              'weight'],\n",
       "             'LOC': ['state', 'other', 'country', 'city', 'mount']})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = defaultdict(lambda: [])\n",
    "\n",
    "for token in label:\n",
    "    mainc, subc = token.split(':')\n",
    "    if not subc in label_dict[mainc]:\n",
    "        label_dict[mainc].append(subc)\n",
    "\n",
    "print(f'main class count: {len(label_dict.keys())}')\n",
    "print(f'subclass count: {sum([len([j for j in i]) for i in label_dict.values()])}')\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest query: 37\n",
      "['Why', 'do', 'people', 'get', 'goosebumps', 'when', 'they', 'have', 'something', 'emotional', 'happen', 'to', 'them', ',', 'like', 'when', 'they', 'hear', 'a', 'beautiful', 'piece', 'of', 'music', ',', 'or', 'see', 'something', 'beautiful', ',', 'or', 'get', 'aroused', 'by', 'someone', 'they', 'love', '?']\n",
      "---\n",
      "Shortest query: 3\n",
      "['Define', 'cosmology', '.']\n"
     ]
    }
   ],
   "source": [
    "query_lengths = [len(q) for q in query]\n",
    "print(f'Longest query: {max(query_lengths)}')\n",
    "print(query[max(zip(query_lengths, range(len(query_lengths))))[1]])\n",
    "print('---')\n",
    "print(f'Shortest query: {min(query_lengths)}')\n",
    "print(query[min(zip(query_lengths, range(len(query_lengths))))[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
